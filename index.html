<!DOCTYPE html>
<html lang="es">
  <head>
    <meta charset="UTF-8" />
    <title>Document Detection MVP</title>
    <!-- 1. Cargar TFJS -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <style>
      .container {
        max-width: 800px;
        margin: 0 auto;
        padding: 20px;
        text-align: center;
      }
      #canvas {
        margin-top: 20px;
        border: 1px solid #ccc;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>Document Detection MVP</h1>
      <video
        id="video"
        width="640"
        height="640"
        autoplay
        playsinline
        muted
        style="display: none"
      ></video>
      <canvas id="canvas" width="640" height="640"></canvas>
    </div>

    <script>
      let model;
      let video;
      let canvas, ctx;

      // Cargar modelo una vez que cargue la página
      window.onload = async () => {
        await loadModel();
        setupCamera();
      };

      async function loadModel() {
        try {
          model = await tf.loadGraphModel("./best_web_model/model.json");
          console.log("Modelo cargado correctamente");
        } catch (error) {
          console.error("Error al cargar el modelo:", error);
        }
      }

      // Solicitar acceso a la cámara y configurar <video>
      async function setupCamera() {
        video = document.getElementById("video");
        canvas = document.getElementById("canvas");
        ctx = canvas.getContext("2d");

        // Solicitar stream de la cámara
        navigator.mediaDevices
          .getUserMedia({ video: true, audio: false })
          .then((stream) => {
            video.srcObject = stream;
            video.onloadedmetadata = () => {
              video.play();
              // Iniciar loop de detección
              detectFrame();
            };
          })
          .catch((err) => {
            console.error("No se pudo acceder a la cámara:", err);
          });
      }

      // Función principal de detección, se llama en cada frame
      async function detectFrame() {
        // Dibujar el frame actual de video en el canvas
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        // Convertir imagen a tensor
        const input = tf.tidy(() => {
          return tf.image
            .resizeBilinear(tf.browser.fromPixels(video), [640, 640])
            .expandDims(0) // [1, 640, 640, 3]
            .div(255); // Normalizar [0,1] - Depende de tu entrenamiento
        });

        // Ejecutar predicción (ojo: algunos modelos usan executeAsync)
        // Ejecutar predicción
        let predictions;
        try {
          predictions = model.execute(input);
        await postProcessOutput(predictions, ctx);

          console.log("Predictions:", predictions); // <-- para depuración
        } catch (err) {
          console.error("Error al ejecutar el modelo:", err);
        }

        // Si 'predictions' es un único tensor:
        const boxes = predictions.arraySync(); // [ [x1,y1,x2,y2,score,classId], [...], ... ]

        // Liberar tensores
        tf.dispose([predictions, input]);

        // Dibuja bounding boxes
        drawBoxes(boxes);

        // Liberar tensores
        tf.dispose([predictions, input]);

        // Dibuja bounding boxes
        drawBoxes(boxes);

        // Solicitar el siguiente frame
        requestAnimationFrame(detectFrame);
      }

      function drawBoxes(boxes) {
        for (let i = 0; i < boxes.length; i++) {
          const [x1, y1, x2, y2, score, classId] = boxes[i];
          const confianza = score.toFixed(2);

          // Filtrar solo si la confianza es suficientemente alta
          if (confianza > 0.5) {
            // Calcular ancho/alto
            const width = x2 - x1;
            const height = y2 - y1;

            // Dibuja rectángulo
            ctx.beginPath();
            ctx.strokeStyle = "red";
            ctx.lineWidth = 2;
            ctx.rect(x1, y1, width, height);
            ctx.stroke();

            // Dibuja texto (confianza o clase)
            ctx.fillStyle = "red";
            ctx.font = "18px Arial";
            ctx.fillText(
              `Doc(${classId}) - ${confianza}`,
              x1,
              Math.max(y1 - 5, 15)
            );
          }
        }
      }

      async function postProcessOutput(rawOutput, ctx) {
        // 1) Quitar dimensión de batch y transponer si es necesario
        let raw = rawOutput.squeeze([0]).transpose([1,0]); // [8400,5]
        const data = await raw.array();

        let boxes = [], scores = [];
        data.forEach(d => {
          const [cx, cy, w, h, conf] = d;
          // 2) Decodificar a (x1, y1, x2, y2) según tu lógica
          //    Aquí se asume que (cx, cy, w, h) ya están en pixeles
          if (conf > 0.25) {
            const x1 = cx - w/2, y1 = cy - h/2;
            const x2 = cx + w/2, y2 = cy + h/2;
            boxes.push([y1, x1, y2, x2]); // tf.image.nonMaxSuppression usa formato [y1,x1,y2,x2]
            scores.push(conf);
          }
        });

        // 4) Aplicar Non-Max Suppression
        const indices = await tf.image.nonMaxSuppressionAsync(
          tf.tensor2d(boxes), tf.tensor1d(scores), 50, 0.5
        );
        const finalIndices = await indices.array();

        // 5) Dibujar cajas
        finalIndices.forEach(i => {
          const [y1, x1, y2, x2] = boxes[i];
          ctx.strokeStyle = "red";
          ctx.strokeRect(x1, y1, x2 - x1, y2 - y1);
          ctx.fillStyle = "red";
          ctx.fillText(scores[i].toFixed(2), x1, y1 - 5);
        });
      }

      async function detectAndDraw() {
        const rawOutput = model.execute(/* tu tensor de entrada */);
      }
    </script>
  </body>
</html>
